# lfi-benchmark

This repo is for pedagogical demonstration. It was prepared for a seminar talk titled "Tools for Scalable and Reproducible Research Pipelines," given 24 September 2025 as part of the **StatBytes Statistical Computing Seminar** for the Department of Statistics and Data Science at Carnegie Mellon University.

In this repo, we demonstrate some research-level statistical computing concepts to which the literature typically refers as **simulator-based inference** (SBI) or **likelihood-free inference** (LFI). The scientific content of the demo is chiefly meant to be sufficiently complex for relevance to practicing computational statisticians, and so the content itself may be of separate interest. We use the **NeurIPS 2024 HiggsML Uncertainty Challenge** benchmark data set from [FAIR Universe](https://fair-universe.lbl.gov/), the original ingestion repo of which has been slightly modified to suit the demo.

Some of the code used in this demonstration was generated by the _Claude_ large language model.

# Structure of this repo

The main branch of this demo repo represents some good practices for scalable and reproducible research software. The structure of this repo mimics typical difficulty-scaling with methodological development for any data analysis-heavy paper:
1. In $\textsc{Parameterize}$, we start small by just writing one script -- it may very well start with a minimal Jupyter notebook, say -- that completes a minimal reproducible proof-of-concept result with a synthetic toy model.
2. In $\textsc{Parallelize}$, we take the minimal example and start pushing and prodding it, so to speak, in directions that help us explore its richness to the extent that phenomena we see may inform our intuition for what will happen when transferring our methodology to a more realistic case study.
3. In $\textsc{Modularize}$, we tackle a realistic case study, drawing on code from internal and external "modules" that will help us maintain scripts as short and readable as were had in the toy examples.

## 1. $\textsc{Parameterize}$

Refer to the `script/gaussian/dev` subdirectory. This folder is all-inclusive -- if you activate the appropriate `conda` environment, say, then the `one_np.py` script can be run from the command line without drawing on code written in other subdirectories within this code repo. Furthermore, it features a typical but bared-down use of the `click` library's command line interface (CLI). This means that you can change the way that the script is run as you submit the script from the command line.

Example:

## 2. $\textsc{Parallelize}$

Refer to the `script/gaussian/batch` subdirectory. In this folder, we have retained `one_np.py` as identical to the one in `../dev`, but this time, there are a few new files:
- A shell script, `one_np.sh`, which lists a sequence of commands which will be automatically interpreted by the terminal as though a user were interfacing with the terminal with that exact sequence, line by line; and
- A text file, `array_params.txt`, each line of which parameterizes a desired iterate of `one_np.py`.

Example:

## 3. $\textsc{Modularize}$

Finally, we assemble a few topics simultaneously in `script/higgs`, namely imports from
- The `src` directory
- The `FAIR_Universe_dataset` submodule

### `FAIR_Universe_dataset` submodule

This repo has an example of a `git submodule`. Upon cloning, your _local_ version of _this_ repo will have a reference to a particular commit of the _remote_ version of the _submodule_ repo. In order to access the content of the submodule in your subsequent scripts, be sure to run

> git submodule init
> git submodule update

to collect the content.

# Research task

TODO: Fill in